% Template:     Informe LaTeX
% Documento:    Archivo principal
% Versi칩n:      8.2.4 (29/04/2023)
% Codificaci칩n: UTF-8
%
% Autor: Pablo Pizarro R.
%        pablo@ppizarror.com
%
% Manual template: [https://latex.ppizarror.com/informe]
% Licencia MIT:    [https://opensource.org/licenses/MIT]

% CREACI칍N DEL DOCUMENTO
\documentclass[
	spanish, % Idioma: spanish, english, etc.
	oneside
]{article}

% INFORMACI칍N DEL DOCUMENTO
\def\documenttitle {Tarea 1}
\def\documentsubtitle {GridWorld}
\def\documentsubject {Aprendizaje por Refuerzo - GridWorld}

\def\documentauthor {Artigues}
\def\coursename {Aprendizaje por Refuerzo}
\def\coursecode {503650}

\def\universityname {Universidad de Concepci칩n}
\def\universityfaculty {Facultad de Ingenier칤a}
\def\universitydepartment {Departamento de Inform치tica y Ciencias de la Computaci칩n}
\def\universitydepartmentimage {departamentos/fiudec2}
\def\universitydepartmentimagecfg {height=1.57cm}
\def\universitylocation {Concepci칩n, Chile}

% INTEGRANTES, PROFESORES Y FECHAS
\def\authortable {
	\begin{tabular}{ll}
		Integrante:
		& \begin{tabular}[t]{l}
			Roberto Felipe Artigues Escobar \\
		\end{tabular} \\ & \\
		Profesor:
		& \begin{tabular}[t]{l}
			Julio Godoy
		\end{tabular} \\
		% Auxiliar:
		% & \begin{tabular}[t]{l}
		% 	Auxiliar 1
		% \end{tabular} \\
		% Ayudantes:
		% & \begin{tabular}[t]{l}
		% 	Ayudante 1 \\
		% 	Ayudante 2
		% \end{tabular} \\
		% \multicolumn{2}{l}{Ayudante de laboratorio: Ayudante 1} \\
		& \\
		% \multicolumn{2}{l}{Fecha de realizaci칩n: \today} \\
		\multicolumn{2}{l}{Fecha de entrega: 12 de Noviembre de 2023} \\
		\multicolumn{2}{l}{\universitylocation}
	\end{tabular}
}

\DeclareUnicodeCharacter{0301}{*************************************}

% IMPORTACI칍N DEL TEMPLATE
\input{template}

% INICIO DE P츼GINAS
\begin{document}

% PORTADA
\templatePortrait

% CONFIGURACION DE P츼GINA Y ENCABEZADOS
\templatePagecfg

% RESUMEN O ABSTRACT
% \begin{abstractd}
% 	% \lipsum[1] % P치rrafo ejemplo, se puede borrar
% 	Se realiz칩 un trabajo experimental en el cual se aplicaron cargas de distintos pesos a una armadura de tipo Warren. Durante el proceso, se midieron los valores de deformaci칩n utilizando un medidor de deformaciones y se mantuvieron registrados mientras se manten칤an las cargas aplicadas. \\ \\
% 	A continuaci칩n, se multiplicaron los valores obtenidos por la constante 洧띺 con el fin de obtener los valores de fuerza correspondientes. Adem치s, se pudo determinar si las barras de la armadura estaban sometidas a tracci칩n o compresi칩n.
% 	\\ \\
% 	Los objetivos del trabajo fueron:  
% 	\begin{itemize}
% 		\item Estudiar el equilibrio de fuerzas en una estructura.
% 		\item Medir fuerza axial en miembros estructurales.
% 		\item Hacer una breve descripci칩n del banco de ensayos, condiciones de carga.
% 		\item Indicar el diagrama de cuerpo libre de la armadura.
% 		\item Realizar una comparaci칩n y discusi칩n de los resultados.
% 	\end{itemize}

% 	\noindent Con respecto a los resultados obtenidos no fueron los esperados, tanto en magnitud de fuerzas como en estados de carga. 
% 	Esto pudo haber ocurrido por fallos en los materiales o por errores al momento de medir las cargas. \\

% 	\noindent En conclusi칩n, es posible afirmar que el an치lisis te칩rico solo representa una aproximaci칩n a la realidad y no tiene en cuenta todos 
% 	los factores que podr칤an influir en los resultados de la experimentaci칩n en condiciones reales, por lo tanto, cabe la posibilidad 
% 	de que los resultados del an치lisis te칩rico y experimental no sean iguales.
	 

% \end{abstractd}

% TABLA DE CONTENIDOS - 칈NDICE
% \templateIndex

% CONFIGURACIONES FINALES
\templateFinalcfg

% ======================= INICIO DEL DOCUMENTO =======================

\section*{Instrucciones de ejecuci칩n del c칩digo}

Todo el c칩digo correspondiente a la tarea se encuentra en la carpeta \textbf{GridWorldEnvs}. Paso por paso se tiene que hacer lo siguiente:

\begin{enumerate}
	\item Abrir una terminal en la carpeta \textbf{GridWorldEnvs}.
	\item Ejecutar el comando "\textbf{pip install -e .}" \space para instalar las dependencias.
	\item Ejecutar el comando "\textbf{python tarea1.py}" \space para ejecutar el c칩digo.
\end{enumerate}

\noindent La raz칩n por la cual se incluye la carpeta completa de \textbf{GridWorldEnvs} es porque se modificaron varios archivos de la librer칤a original. \\

\noindent Al final del c칩digo hay varias secciones comentadas, las cuales se pueden descomentar para ejecutar los experimentos requeridos para responder cada pregunta. \\

\vspace*{12pt}
\hrule height 0.1pt 
\vspace*{20pt}

\noindent Tambi칠n se incluye el archivo \textbf{plotter.py} para graficar los resultados obtenidos. Para ejecutarlo, se debe hacer lo siguiente:

\begin{enumerate}
	\item Descargar matplotlib con el comando "\textbf{pip install matplotlib}".
	\item Ejecutar el comando "\textbf{python plotter.py}" \space para obtener los gr치ficos.
\end{enumerate}

\noindent Los gr치ficos se crean usando los archivos de rewards en la carpeta \textbf{GridWorldEnvs/Rewards}. Se tiene que especificar el archivo a usar en el c칩digo del programa. \\

\newpage
\section*{Pregunta 1}

\noindent A continuaci칩n se muestran los resultados obtenidos con SARSA y Q-learning para el Map1. 
\\ \\ 
\indent{\textbf{Resultados con Map1:}}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.96\textwidth]{img/SarsaDefault1.png}
	\caption{Resultados con SARSA para Map1}
	\label{fig:map1_sarsa}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.96\textwidth]{img/QlearningDefault1.png}
	\caption{Resultados con Q-learning para Map1}
	\label{fig:map1_qlearning}
\end{figure}


\newpage
\indent{\textbf{Resultados con Map2:}}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/SarsaDefault2.png}
	\caption{Resultados con SARSA para Map2}
	\label{fig:map2_sarsa}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/QlearningDefault2.png}
	\caption{Resultados con Q-learning para Map2}
	\label{fig:map2_qlearning}
\end{figure}

\noindent Con los par치metros originales no es posible que el agente aprenda a llegar al objetivo en Map2, ya que el agente se queda dando vueltas en el mapa sin llegar al objetivo. \\


\indent{\textbf{Resultados con Map2 y parametros modificados:}}
\\

\noindent Los valores que se utilizaron en los par치metros para los resultados de la Figura \ref{fig:map2_sarsa_modified} y la Figura \ref{fig:map2_qlearning_modified} se muestran en la Tabla \ref{tab:my-table}.
\\
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \rowcolor[HTML]{C0C0C0} 
        \textbf{Par치metro} & \textbf{Original} & \multicolumn{2}{c|}{\textbf{Modificado}} \\ \hline
        \textbf{MAX\_STEPS} & 100 & \multicolumn{2}{c|}{200} \\ \hline
        \textbf{LEARNING\_RATE} & 0.2 & \multicolumn{2}{c|}{0.1} \\ \hline
        \textbf{GAMMA} & 0.9 & 0.95 & 1 \\ \hline
        \textbf{EPSILON} & 1 & \multicolumn{2}{c|}{1} \\ \hline
    \end{tabular}
    \caption{Valores de par치metros utilizados}
    \label{tab:my-table}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/SarsaModified2g095.png}
	\caption{Resultados con SARSA para Map2 con gamma = 0.95}
	\label{fig:map2_sarsa_modified}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/QlearningModified2g095.png}
	\caption{Resultados con Q-learning para Map2 con gamma = 0.95}
	\label{fig:map2_qlearning_modified}
\end{figure}

\noindent Al menos SARSA no siempre encontraba la soluci칩n 칩ptima. En cambio, Q-learning siempre encontraba la soluci칩n 칩ptima usando los par치metros modificados. \\

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/SarsaModified2g1.png}
	\caption{Resultados con SARSA para Map2 con gamma = 1}
	\label{fig:map2_sarsa_modified2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/QlearningModified2g1.png}
	\caption{Resultados con Q-learning para Map2 con gamma = 1}
	\label{fig:map2_qlearning_modified2}
\end{figure}

\noindent La modificaci칩n de los par치metros tuvo un impacto significativo en el desempe침o de los algoritmos SARSA y Q-learning en el Map2. Con un valor de \textbf{gamma} igual a 0.95, se observ칩 que SARSA no siempre encontraba la soluci칩n 칩ptima. Por otro lado, Q-learning mostr칩 una mejora notable, encontrando consistentemente la soluci칩n 칩ptima.

\vspace*{12pt}
\noindent Al incrementar el valor de \textbf{gamma} a 1, se intensifica la importancia de las recompensas futuras. Por lo que ambos algoritmos deciden tomar el camino m치s corto hacia el objetivo, sin importar las recompensas que se obtengan en el camino. \\

\newpage

\section*{Pregunta 2}

Para el entrenamiento, se volvieron a utilizar los par치metros modificados, espec칤ficamente utilizando \textbf{gamma} = 1. \\

\indent\textbf{Q-learning determin칤stico vs estoc치stico}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/QlearningModified2g1.png}
	\caption{Resultados Q-learning determin칤stico para Map2 con gamma = 1}
	\label{fig:map2_qlearning_modified22}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/QlearningStochastic2g1.png}
	\caption{Resultados Q-learning estoc치stico para Map2 con gamma = 1}
	\label{fig:map2_qlearning_stochastic}
\end{figure}


\indent\textbf{SARSA determin칤stico vs estoc치stico}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/SarsaModified2g1.png}
	\caption{Resultados SARSA determin칤stico para Map2 con gamma = 1}
	\label{fig:map2_sarsa_modified22}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/SarsaStochastic2g1.png}
	\caption{Resultados SARSA estoc치stico para Map2 con gamma = 1}
	\label{fig:map2_sarsa_stochastic}
\end{figure}

\newpage
\noindent Ambos algoritmos, SARSA y Q-learning, lograron encontrar soluciones efectivas en una cantidad de episodios similar, aunque la aleatoriedad inherente a los entornos estoc치sticos impact칩 claramente sus curvas de aprendizaje. Esta aleatoriedad introduce una variabilidad en las recompensas y trayectorias que los algoritmos deben aprender a navegar.
\vspace*{12pt}

\noindent En particular, Q-learning, que es un algoritmo off-policy, tiende a verse m치s afectado por esta aleatoriedad. Al aprender una pol칤tica 칩ptima mientras explora otras opciones, la variabilidad en los resultados de las acciones puede llevar a una adaptaci칩n m치s lenta. Por otro lado, SARSA, siendo un algoritmo on-policy, aprende directamente de la pol칤tica actual que est치 siguiendo, lo que le permite adaptarse m치s r치pidamente a los cambios y variaciones del entorno. Esto significa que SARSA puede responder de manera m치s efectiva a la aleatoriedad en los movimientos, ajustando su pol칤tica de aprendizaje de forma continua y coherente con la experiencia directa.
\vspace*{12pt}

\noindent Esta diferencia en la adaptabilidad ante la aleatoriedad subraya la importancia de elegir el algoritmo adecuado en funci칩n de las caracter칤sticas espec칤ficas del entorno en el que se va a operar. Mientras que Q-learning busca la soluci칩n 칩ptima sin considerar la pol칤tica actual, SARSA integra la experiencia real en su proceso de aprendizaje, lo que puede ser ventajoso en entornos donde la previsibilidad no est치 garantizada.

\newpage
\section*{Pregunta 3}

\noindent Se mostraran los resultados entre Q-learning y Double Q-learning para Map2, usando \textbf{gamma} = 1. Primero en un ambiente determin칤stico y luego en un ambiente estoc치stico. \\


\indent\textbf{Q-learning determin칤stico vs Double Q-learning determin칤stico}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/QlearningModified2g1.png}
	\caption{Resultados Q-learning determin칤stico}
	\label{fig:map2_qlearning_modified23}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/DoubleQlearningg1.png}
	\caption{Resultados Double Q-learning determin칤stico}
	\label{fig:map2_double_qlearning_modified}
\end{figure}

\newpage

\indent\textbf{Q-learning estoc치stico vs Double Q-learning estoc치stico}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/QlearningStochastic2g1.png}
	\caption{Resultados Q-learning estoc치stico}
	\label{fig:map2_qlearning_stochastic2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/DoubleQlearningStochastic2g1.png}
	\caption{Resultados Double Q-learning estoc치stico}
	\label{fig:map2_double_qlearning_stochastic}
\end{figure}

\newpage
\noindent En un entorno determin칤stico, Double Q-learning no mostr칩 una mejora significativa en comparaci칩n con Q-learning. Esto se debe a que en un entorno determin칤stico, Q-learning ya puede aprender la pol칤tica 칩ptima sin sobrestimar los valores de acci칩n. \\
\vspace*{12pt}

\noindent Por otro lado, en un entorno estoc치stico, Double Q-learning mostr칩 una mejora significativa en comparaci칩n con Q-learning. Esto se debe a que en un entorno estoc치stico, Q-learning sobrestima los valores de acci칩n, lo que puede llevar a una pol칤tica sub칩ptima. Double Q-learning puede evitar este problema, ya que utiliza dos funciones de valor para estimar y reducir la sobreestimaci칩n. \\

\newpage
\section*{Pregunta 4}

\noindent Se comparara el desempe침o de SARSA y Q-learning originales a la version con \textbf{eligibility traces} para Map2, usando \textbf{gamma} = 1 y \textbf{lambda} = 0.5. En un ambiente estoc치stico. \\

\indent\textbf{SARSA vs SARSA con eligibility traces}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/SarsaStochastic2g1.png}
	\caption{Resultados SARSA estoc치stico}
	\label{fig:map2_sarsa_stochastic2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/SarsaEligibilityStochastic2g1.png}
	\caption{Resultados SARSA con eligibility traces estoc치stico}
	\label{fig:map2_sarsa_eligibility_stochastic}
\end{figure}

\newpage
\indent\textbf{Q-learning vs Q-learning con eligibility traces}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/QlearningStochastic2g1.png}
	\caption{Resultados Q-learning estoc치stico}
	\label{fig:map2_qlearning_stochastic3}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{img/QlearningEligibilityStochastic2g1.png}
	\caption{Resultados Q-learning con eligibility traces estoc치stico}
	\label{fig:map2_qlearning_eligibility_stochastic}
\end{figure}

\newpage
\noindent En ambos casos, el rendimiento de los algoritmos SARSA y Q-learning se vio afectado negativamente con la introducci칩n de trazas de elegibilidad. En el caso de SARSA, a pesar del impacto en el rendimiento general, el algoritmo a칰n logra alcanzar el objetivo de manera relativamente consistente y r치pida. Esta capacidad podr칤a atribuirse a su naturaleza on-policy, que le permite ajustar mejor su pol칤tica a la experiencia directa y reciente, incluso con la complejidad a침adida de las trazas de elegibilidad.
\vspace*{12pt}

\noindent Por otro lado, Q-learning, siendo un algoritmo off-policy, parece tener dificultades significativas en este escenario. La combinaci칩n de aprendizaje off-policy con trazas de elegibilidad podr칤a estar llevando al algoritmo a una especie de paradoja de decisi칩n, donde el deseo de optimizar recompensas a largo plazo lo aleja del objetivo. 



% FIN DEL DOCUMENTO
\end{document}